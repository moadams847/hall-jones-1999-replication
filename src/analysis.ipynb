{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c0611c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hall & Jones (1999) Replication — Analysis Script\n",
    "==================================================\n",
    "Produces:\n",
    "  - Table I  : Levels accounting decomposition\n",
    "  - Table II : IV regression (OLS + 2SLS, robust SEs)\n",
    "  - Figure I : Y/L vs social infrastructure scatter\n",
    "  - Figure II: TFP vs social infrastructure scatter\n",
    "\n",
    "Two samples throughout:\n",
    "  (A) Complete cases only  — 93 countries, no imputation\n",
    "  (B) Imputed sample       — ~127 countries, missing openness/governance\n",
    "                             imputed via OLS on instruments (Hall & Jones method)\n",
    "\n",
    "Run from any directory. Reads hj_master.csv from outputs folder.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ── paths ──────────────────────────────────────────────────────────────────\n",
    "MASTER  = 'C:\\\\Users\\\\Adams\\\\OneDrive\\\\DE & E Research\\\\outputs\\\\merged.csv'\n",
    "OUT_DIR = 'C:\\\\Users\\\\Adams\\\\OneDrive\\\\DE & E Research\\\\outputs\\\\'\n",
    "\n",
    "ALPHA = 1/3   # capital share (Hall & Jones assumption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "189eddc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "  Hall & Jones (1999) Replication\n",
      "=================================================================\n",
      "\n",
      "Loaded 183 countries from master dataset\n",
      "Sample A (complete cases):  89 countries\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 1 — LOAD AND PREPARE DATA\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "print(\"=\" * 65)\n",
    "print(\"  Hall & Jones (1999) Replication\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "df = pd.read_csv(MASTER, encoding='latin1')\n",
    "print(f\"\\nLoaded {len(df)} countries from master dataset\")\n",
    "\n",
    "# ── 1a. Mining correction ──────────────────────────────────────────────────\n",
    "# Hall & Jones subtract mining value added from GDP before the accounting.\n",
    "# If mining share is missing, assume 0 (conservative — documented deviation).\n",
    "df['mining_va'] = df['mining_va'].fillna(0.0)\n",
    "\n",
    "# GDP net of mining (fraction of total GDP remaining after removing mining VA)\n",
    "df['gdp_nonmining_share'] = 1.0 - df['mining_va']\n",
    "\n",
    "# Adjust Y/L for mining: multiply by non-mining share\n",
    "# (If mining is 30% of GDP, we want Y/L to reflect only the non-mining economy)\n",
    "df['yl_adj'] = df['yl'] * df['gdp_nonmining_share']\n",
    "\n",
    "# ── 1b. Core variables for accounting ────────────────────────────────────\n",
    "# log output per worker (adjusted for mining)\n",
    "df['log_yl']  = np.log(df['yl_adj'])\n",
    "\n",
    "# Capital term: (K/Y)^(alpha/(1-alpha))\n",
    "# With alpha=1/3: exponent = (1/3)/(2/3) = 0.5  → square root of K/Y\n",
    "df['cap_term'] = df['ky_ratio'] ** (ALPHA / (1 - ALPHA))\n",
    "\n",
    "# Human capital h already computed (Mincerian index)\n",
    "df['hc_term'] = df['hc']\n",
    "\n",
    "# TFP residual A = (Y/L) / [cap_term × hc_term]\n",
    "# i.e. Y/L = cap_term × hc_term × A  →  A = Y/L / (cap_term × hc_term)\n",
    "df['tfp'] = df['yl_adj'] / (df['cap_term'] * df['hc_term'])\n",
    "\n",
    "# log versions for scatter plots\n",
    "df['log_si']  = np.log(df['social_infra'].clip(lower=1e-6))\n",
    "df['log_tfp'] = np.log(df['tfp'].clip(lower=1e-6))\n",
    "\n",
    "# ── 1c. Instrument list ──────────────────────────────────────────────────\n",
    "INSTRUMENTS = ['distancefromeq', 'fr_trade', 'english_frac', 'we_lang_frac']\n",
    "\n",
    "# ── 1d. Complete-case sample (Sample A) ──────────────────────────────────\n",
    "core_vars = ['yl_adj', 'ky_ratio', 'hc', 'social_infra',\n",
    "             'distancefromeq', 'fr_trade', 'english_frac', 'we_lang_frac']\n",
    "sA = df.dropna(subset=core_vars).copy().reset_index(drop=True)\n",
    "print(f\"Sample A (complete cases):  {len(sA)} countries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0ad0e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Imputing missing social infrastructure ---\n",
      "  Imputed social infrastructure for 38 additional countries\n",
      "  Imputation regression R²: 1.000\n",
      "Sample B (with imputation): 113 countries\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 2 — IMPUTATION (Hall & Jones method)\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "print(\"\\n--- Imputing missing social infrastructure ---\")\n",
    "\n",
    "# Hall & Jones impute missing governance/openness by regressing each component\n",
    "# on the four instruments + quadratic distance from equator.\n",
    "# We apply the same logic to social_infra directly (since we already\n",
    "# combined governance and openness into one index).\n",
    "\n",
    "def ols_impute(df_full, target_col, instrument_cols):\n",
    "    \"\"\"\n",
    "    Regress target_col on instrument_cols (+ dist^2) using complete cases,\n",
    "    then predict for all rows. Returns a new Series with imputed values.\n",
    "    \"\"\"\n",
    "    # Add quadratic distance term\n",
    "    regressors = instrument_cols + ['distancefromeq_sq']\n",
    "    df_full = df_full.copy()\n",
    "    df_full['distancefromeq_sq'] = df_full['distancefromeq'] ** 2\n",
    "\n",
    "    # Training set: rows where both target and all regressors are observed\n",
    "    train = df_full.dropna(subset=[target_col] + regressors)\n",
    "    \n",
    "    X_train = np.column_stack([np.ones(len(train))] +\n",
    "                               [train[c].values for c in regressors])\n",
    "    y_train = train[target_col].values\n",
    "\n",
    "    # OLS: beta = (X'X)^{-1} X'y\n",
    "    beta = np.linalg.lstsq(X_train, y_train, rcond=None)[0]\n",
    "\n",
    "    # Predict for all rows that have complete instruments\n",
    "    pred_col = target_col + '_imputed'\n",
    "    df_full[pred_col] = np.nan\n",
    "    has_inst = df_full.dropna(subset=regressors)\n",
    "    X_pred = np.column_stack([np.ones(len(has_inst))] +\n",
    "                               [has_inst[c].values for c in regressors])\n",
    "    preds = X_pred @ beta\n",
    "    # Clip predictions to [0, 1] (social infrastructure is bounded)\n",
    "    preds = np.clip(preds, 0.0, 1.0)\n",
    "    df_full.loc[has_inst.index, pred_col] = preds\n",
    "\n",
    "    return df_full[pred_col], beta\n",
    "\n",
    "# Impute social infrastructure\n",
    "si_imputed, beta_si = ols_impute(df, 'social_infra', INSTRUMENTS)\n",
    "\n",
    "# Build imputed series: use observed value if available, else imputed\n",
    "df['social_infra_imp'] = df['social_infra'].combine_first(si_imputed)\n",
    "\n",
    "n_imputed = df['social_infra'].isna().sum() - df['social_infra_imp'].isna().sum()\n",
    "print(f\"  Imputed social infrastructure for {n_imputed} additional countries\")\n",
    "print(f\"  Imputation regression R²: {stats.pearsonr(df.dropna(subset=['social_infra','social_infra_imp'])['social_infra'], df.dropna(subset=['social_infra','social_infra_imp'])['social_infra_imp'])[0]**2:.3f}\")\n",
    "\n",
    "# ── Sample B: imputed sample ──────────────────────────────────────────────\n",
    "core_vars_B = ['yl_adj', 'ky_ratio', 'hc', 'social_infra_imp',\n",
    "               'distancefromeq', 'fr_trade', 'english_frac', 'we_lang_frac']\n",
    "sB = df.dropna(subset=core_vars_B).copy().reset_index(drop=True)\n",
    "sB['social_infra'] = sB['social_infra_imp']\n",
    "sB['log_si']  = np.log(sB['social_infra'].clip(lower=1e-6))\n",
    "sB['tfp']     = sB['yl_adj'] / (sB['cap_term'] * sB['hc_term'])\n",
    "sB['log_tfp'] = np.log(sB['tfp'].clip(lower=1e-6))\n",
    "sB['log_yl']  = np.log(sB['yl_adj'])\n",
    "print(f\"Sample B (with imputation): {len(sB)} countries\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d834e58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 3 — ECONOMETRICS TOOLKIT (OLS and 2SLS from scratch)\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def ols(y, X):\n",
    "    \"\"\"\n",
    "    OLS regression.\n",
    "    Returns dict: beta, residuals, fitted, n, k, HC1 robust covariance matrix.\n",
    "    X should include a constant column.\n",
    "    \"\"\"\n",
    "    n, k = X.shape\n",
    "    XX_inv = np.linalg.inv(X.T @ X)\n",
    "    beta   = XX_inv @ X.T @ y\n",
    "    yhat   = X @ beta\n",
    "    e      = y - yhat\n",
    "\n",
    "    # HC1 (heteroskedasticity-robust) covariance: (X'X)^{-1} X' diag(e²) X (X'X)^{-1} * n/(n-k)\n",
    "    meat   = X.T @ np.diag(e**2) @ X\n",
    "    V_hc1  = (n / (n - k)) * XX_inv @ meat @ XX_inv\n",
    "\n",
    "    ss_res = e @ e\n",
    "    ss_tot = ((y - y.mean()) ** 2).sum()\n",
    "    r2     = 1 - ss_res / ss_tot\n",
    "\n",
    "    return {'beta': beta, 'e': e, 'yhat': yhat,\n",
    "            'n': n, 'k': k, 'V': V_hc1, 'r2': r2}\n",
    "\n",
    "\n",
    "def tsls(y, X_endog, X_exog, Z):\n",
    "    \"\"\"\n",
    "    Two-Stage Least Squares (2SLS / IV).\n",
    "\n",
    "    y       : outcome (n,)\n",
    "    X_endog : endogenous regressors (n, p) — social infrastructure here\n",
    "    X_exog  : included exogenous regressors (n, q) — just [1] (constant) here\n",
    "    Z       : excluded instruments (n, m) — the four geographic/language vars\n",
    "\n",
    "    Returns same dict as ols() plus first-stage F-stat and overid test.\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "\n",
    "    # Full instrument matrix for first stage\n",
    "    Z_full = np.column_stack([X_exog, Z])   # [const, instruments]\n",
    "    X_full = np.column_stack([X_exog, X_endog])  # [const, S]\n",
    "\n",
    "    # First stage: regress S on [const, instruments]\n",
    "    fs = ols(X_endog.ravel(), Z_full)\n",
    "    S_hat = fs['yhat'].reshape(-1, 1)\n",
    "\n",
    "    # First-stage F-statistic (robust)\n",
    "    # Test that all instrument coefficients are jointly zero\n",
    "    # Positions of instrument coefficients: columns 1 onwards in Z_full\n",
    "    n_inst = Z.shape[1]\n",
    "    R = np.zeros((n_inst, Z_full.shape[1]))\n",
    "    for i in range(n_inst):\n",
    "        R[i, i + X_exog.shape[1]] = 1.0\n",
    "    r = np.zeros(n_inst)\n",
    "    Rb_r = R @ fs['beta'] - r\n",
    "    V_fs  = fs['V']\n",
    "    F_stat = (Rb_r @ np.linalg.inv(R @ V_fs @ R.T) @ Rb_r) / n_inst\n",
    "\n",
    "    # Second stage: regress y on [const, S_hat]\n",
    "    X2 = np.column_stack([X_exog, S_hat])\n",
    "    ss = ols(y, X2)\n",
    "\n",
    "    # Correct 2SLS standard errors:\n",
    "    # Use actual residuals (y - X_full @ beta_2sls), not second-stage residuals\n",
    "    beta_2sls = ss['beta']\n",
    "    e_2sls    = y - X_full @ beta_2sls\n",
    "\n",
    "    # HC1 robust covariance using actual residuals\n",
    "    XX_inv_2 = np.linalg.inv(X2.T @ X2)\n",
    "    meat_2   = X2.T @ np.diag(e_2sls**2) @ X2\n",
    "    k2       = X2.shape[1]\n",
    "    V_2sls   = (n / (n - k2)) * XX_inv_2 @ meat_2 @ XX_inv_2\n",
    "\n",
    "    ss_res  = e_2sls @ e_2sls\n",
    "    ss_tot  = ((y - y.mean())**2).sum()\n",
    "    r2_2sls = 1 - ss_res / ss_tot\n",
    "\n",
    "    # Sargan overidentification test (if more instruments than endogenous vars)\n",
    "    # Regress 2SLS residuals on full instrument matrix; n*R² ~ chi²(m-p)\n",
    "    overid_p = np.nan\n",
    "    if n_inst > 1:\n",
    "        sar = ols(e_2sls, Z_full)\n",
    "        sargan_stat = n * sar['r2']\n",
    "        df_sar = n_inst - X_endog.shape[1] if X_endog.ndim > 1 else n_inst - 1\n",
    "        overid_p = 1 - stats.chi2.cdf(sargan_stat, df=df_sar)\n",
    "\n",
    "    return {'beta': beta_2sls, 'e': e_2sls, 'yhat': ss['yhat'],\n",
    "            'n': n, 'k': k2, 'V': V_2sls, 'r2': r2_2sls,\n",
    "            'fs_beta': fs['beta'], 'fs_V': V_fs, 'F_first': F_stat,\n",
    "            'overid_p': overid_p}\n",
    "\n",
    "\n",
    "def fmt_result(res, labels):\n",
    "    \"\"\"Print a formatted regression result block.\"\"\"\n",
    "    beta = res['beta']\n",
    "    V    = res['V']\n",
    "    se   = np.sqrt(np.diag(V))\n",
    "    t    = beta / se\n",
    "    p    = 2 * (1 - stats.t.cdf(np.abs(t), df=res['n'] - res['k']))\n",
    "\n",
    "    lines = []\n",
    "    for i, lbl in enumerate(labels):\n",
    "        stars = '***' if p[i] < 0.01 else ('**' if p[i] < 0.05 else\n",
    "                ('*' if p[i] < 0.10 else ''))\n",
    "        lines.append(f\"  {lbl:<22} {beta[i]:>10.4f}  ({se[i]:.4f}) {stars}\")\n",
    "    lines.append(f\"  {'N':<22} {res['n']:>10}\")\n",
    "    lines.append(f\"  {'R²':<22} {res['r2']:>10.4f}\")\n",
    "    if 'F_first' in res:\n",
    "        lines.append(f\"  {'First-stage F':<22} {res['F_first']:>10.2f}\")\n",
    "    if 'overid_p' in res and not np.isnan(res['overid_p']):\n",
    "        lines.append(f\"  {'Overid test p-val':<22} {res['overid_p']:>10.3f}\")\n",
    "    return '\\n'.join(lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "471fb73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================================================\n",
      "  TABLE I — Levels Accounting Decomposition\n",
      "=================================================================\n",
      "  Methodology: Y/L = (K/Y)^(α/(1-α)) × h × A,  α = 1/3\n",
      "  All values relative to the United States = 1.00\n",
      "\n",
      "\n",
      "  [Sample A — complete cases, no imputation]  N = 89\n",
      "\n",
      "  Country                        Y/L   Cap term        h    TFP (A)         S\n",
      "  ----------------------------------------------------------------------\n",
      "  United States                1.000      1.000    1.000      1.000     0.917\n",
      "  Switzerland                  0.894      1.290    0.887      0.782     0.947\n",
      "  Canada                       0.849      0.970    0.855      1.024     0.907\n",
      "  France                       0.813      1.065    0.656      1.165     0.793\n",
      "  Australia                    0.802      1.053    0.927      0.821     0.759\n",
      "  Japan                        0.639      1.073    0.822      0.725     0.738\n",
      "  Colombia                     0.290      0.929    0.546      0.572     0.298\n",
      "  Brazil                       0.204      0.846    0.477      0.506     0.278\n",
      "  Thailand                     0.125      0.753    0.456      0.364     0.785\n",
      "  Kenya                        0.078      0.614    0.446      0.284     0.275\n",
      "  China                        0.060      0.814    0.545      0.135     0.224\n",
      "  India                        0.042      0.792    0.406      0.129     0.250\n",
      "  Nigeria                      0.038      2.664    0.359      0.040     0.154\n",
      "  Niger                        0.035      0.939    0.317      0.119     0.172\n",
      "\n",
      "  90th/10th percentile ratios:\n",
      "    Y/L:      18.9x\n",
      "    Cap term: 2.0x\n",
      "    h:        2.3x\n",
      "    TFP (A):  7.5x\n",
      "\n",
      "  Variance decomposition (fraction of var(log Y/L) explained):\n",
      "    Capital term: 0.129\n",
      "    Education h:  0.237\n",
      "    TFP (A):      0.645\n",
      "    Sum:          1.011\n",
      "\n",
      "  [Sample B — with imputation]  N = 113\n",
      "\n",
      "  Country                        Y/L   Cap term        h    TFP (A)         S\n",
      "  ----------------------------------------------------------------------\n",
      "  United States                1.000      1.000    1.000      1.000     0.917\n",
      "  Switzerland                  0.894      1.290    0.887      0.782     0.947\n",
      "  Canada                       0.849      0.970    0.855      1.024     0.907\n",
      "  France                       0.813      1.065    0.656      1.165     0.793\n",
      "  Australia                    0.802      1.053    0.927      0.821     0.759\n",
      "  United Kingdom               0.695      0.981    0.775      0.914     0.887\n",
      "  Japan                        0.639      1.073    0.822      0.725     0.738\n",
      "  Germany                      0.633      1.105    0.721      0.795     0.691\n",
      "  Colombia                     0.290      0.929    0.546      0.572     0.298\n",
      "  Brazil                       0.204      0.846    0.477      0.506     0.278\n",
      "  Thailand                     0.125      0.753    0.456      0.364     0.785\n",
      "  Kenya                        0.078      0.614    0.446      0.284     0.275\n",
      "  China                        0.060      0.814    0.545      0.135     0.224\n",
      "  India                        0.042      0.792    0.406      0.129     0.250\n",
      "  Nigeria                      0.038      2.664    0.359      0.040     0.154\n",
      "  Niger                        0.035      0.939    0.317      0.119     0.172\n",
      "\n",
      "  90th/10th percentile ratios:\n",
      "    Y/L:      18.2x\n",
      "    Cap term: 2.1x\n",
      "    h:        2.2x\n",
      "    TFP (A):  7.4x\n",
      "\n",
      "  Variance decomposition (fraction of var(log Y/L) explained):\n",
      "    Capital term: 0.124\n",
      "    Education h:  0.226\n",
      "    TFP (A):      0.658\n",
      "    Sum:          1.009\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 4 — TABLE I: LEVELS ACCOUNTING\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\"  TABLE I — Levels Accounting Decomposition\")\n",
    "print(\"=\" * 65)\n",
    "print(\"  Methodology: Y/L = (K/Y)^(α/(1-α)) × h × A,  α = 1/3\")\n",
    "print(\"  All values relative to the United States = 1.00\")\n",
    "print()\n",
    "\n",
    "def levels_accounting(sample, label):\n",
    "    s = sample.copy()\n",
    "\n",
    "    # Normalise everything to US = 1\n",
    "    usa = s[s['iso3'] == 'USA'].iloc[0]\n",
    "    s['rel_yl']  = s['yl_adj']   / usa['yl_adj']\n",
    "    s['rel_cap'] = s['cap_term'] / usa['cap_term']\n",
    "    s['rel_hc']  = s['hc_term']  / usa['hc_term']\n",
    "    s['rel_tfp'] = s['tfp']      / usa['tfp']\n",
    "\n",
    "    # Variance decomposition (in logs, following Hall & Jones eq. 6)\n",
    "    # var(log Y/L) = cov(log Y/L, log cap) + cov(log Y/L, log h) + cov(log Y/L, log A)\n",
    "    log_yl  = np.log(s['rel_yl'].clip(1e-6))\n",
    "    log_cap = np.log(s['rel_cap'].clip(1e-6))\n",
    "    log_hc  = np.log(s['rel_hc'].clip(1e-6))\n",
    "    log_tfp_r = np.log(s['rel_tfp'].clip(1e-6))\n",
    "\n",
    "    var_yl  = np.var(log_yl)\n",
    "    cov_cap = np.cov(log_yl, log_cap)[0, 1]\n",
    "    cov_hc  = np.cov(log_yl, log_hc)[0, 1]\n",
    "    cov_tfp = np.cov(log_yl, log_tfp_r)[0, 1]\n",
    "\n",
    "    share_cap = cov_cap / var_yl\n",
    "    share_hc  = cov_hc  / var_yl\n",
    "    share_tfp = cov_tfp / var_yl\n",
    "\n",
    "    # Range: ratio of 90th to 10th percentile\n",
    "    p90_p10 = lambda x: np.percentile(x, 90) / np.percentile(x, 10)\n",
    "\n",
    "    print(f\"\\n  [{label}]  N = {len(s)}\")\n",
    "    print(f\"\\n  {'Country':<25} {'Y/L':>8} {'Cap term':>10} {'h':>8} {'TFP (A)':>10}  {'S':>8}\")\n",
    "    print(f\"  {'-'*70}\")\n",
    "\n",
    "    # Show 15 representative countries (sorted by Y/L)\n",
    "    show = s.sort_values('rel_yl', ascending=False)\n",
    "    display_isos = ['USA','CHE','CAN','AUS','GBR','JPN','FRA','DEU',\n",
    "                    'BRA','COL','THA','CHN','IND','KEN','NGA','NER']\n",
    "    show_rows = show[show['iso3'].isin(display_isos)]\n",
    "    for _, row in show_rows.iterrows():\n",
    "        si_val = f\"{row['social_infra']:.3f}\" if not pd.isna(row['social_infra']) else '  NA '\n",
    "        print(f\"  {row['country']:<25} {row['rel_yl']:>8.3f} {row['rel_cap']:>10.3f} \"\n",
    "              f\"{row['rel_hc']:>8.3f} {row['rel_tfp']:>10.3f}  {si_val:>8}\")\n",
    "\n",
    "    print(f\"\\n  90th/10th percentile ratios:\")\n",
    "    print(f\"    Y/L:      {p90_p10(s['rel_yl']):.1f}x\")\n",
    "    print(f\"    Cap term: {p90_p10(s['rel_cap']):.1f}x\")\n",
    "    print(f\"    h:        {p90_p10(s['rel_hc']):.1f}x\")\n",
    "    print(f\"    TFP (A):  {p90_p10(s['rel_tfp']):.1f}x\")\n",
    "\n",
    "    print(f\"\\n  Variance decomposition (fraction of var(log Y/L) explained):\")\n",
    "    print(f\"    Capital term: {share_cap:.3f}\")\n",
    "    print(f\"    Education h:  {share_hc:.3f}\")\n",
    "    print(f\"    TFP (A):      {share_tfp:.3f}\")\n",
    "    print(f\"    Sum:          {share_cap + share_hc + share_tfp:.3f}\")\n",
    "\n",
    "    return s  # return enriched sample with rel_ columns\n",
    "\n",
    "sA_enriched = levels_accounting(sA, \"Sample A — complete cases, no imputation\")\n",
    "sB_enriched = levels_accounting(sB, \"Sample B — with imputation\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91f1b88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================================================\n",
      "  TABLE II — OLS and 2SLS Regressions\n",
      "  Dependent variable: log(Y/L)\n",
      "  Key regressor: Social infrastructure S ∈ [0,1]\n",
      "  Robust (HC1) standard errors in parentheses\n",
      "=================================================================\n",
      "\n",
      "\n",
      "  [Sample A — complete cases]  N = 89\n",
      "\n",
      "  OLS:\n",
      "  Constant                   8.1611  (0.1601) ***\n",
      "  Social infra (S)           3.1754  (0.2461) ***\n",
      "  N                              89\n",
      "  R²                         0.5994\n",
      "\n",
      "  2SLS (instruments: dist_eq, FR trade, English frac, WE lang frac):\n",
      "  Constant                   7.6115  (0.2119) ***\n",
      "  Social infra (S)           4.4174  (0.4295) ***\n",
      "  N                              89\n",
      "  R²                         0.5077\n",
      "  First-stage F               16.20\n",
      "  Overid test p-val           0.089\n",
      "\n",
      "  2SLS sensitivity — instrument subsets:\n",
      "  Instruments                           β̂ (S)        SE    F-stat   Overid p\n",
      "  ---------------------------------------------------------------------------\n",
      "  dist_eq only                           4.935    0.6016     33.18        n/a\n",
      "  dist + FR trade                        4.374    0.4861     31.55      0.056\n",
      "  dist + language                        4.863    0.4950     14.96      0.245\n",
      "  all 4 instruments                      4.417    0.4295     16.20      0.089\n",
      "\n",
      "  [Sample B — with imputation]  N = 113\n",
      "\n",
      "  OLS:\n",
      "  Constant                   8.1832  (0.1514) ***\n",
      "  Social infra (S)           3.1560  (0.2299) ***\n",
      "  N                             113\n",
      "  R²                         0.5596\n",
      "\n",
      "  2SLS (instruments: dist_eq, FR trade, English frac, WE lang frac):\n",
      "  Constant                   7.7408  (0.1892) ***\n",
      "  Social infra (S)           4.1184  (0.3669) ***\n",
      "  N                             113\n",
      "  R²                         0.5076\n",
      "  First-stage F               33.30\n",
      "  Overid test p-val           0.056\n",
      "\n",
      "  2SLS sensitivity — instrument subsets:\n",
      "  Instruments                           β̂ (S)        SE    F-stat   Overid p\n",
      "  ---------------------------------------------------------------------------\n",
      "  dist_eq only                           4.749    0.4947     51.71        n/a\n",
      "  dist + FR trade                        4.130    0.3944     60.04      0.026\n",
      "  dist + language                        4.549    0.4020     24.39      0.171\n",
      "  all 4 instruments                      4.118    0.3669     33.30      0.056\n",
      "\n",
      "=================================================================\n",
      "  OLS vs 2SLS comparison (Social infrastructure coefficient β)\n",
      "=================================================================\n",
      "\n",
      "  Sample                            OLS β    2SLS β   Ratio IV/OLS\n",
      "  -----------------------------------------------------------------\n",
      "  A — complete cases                3.175     4.417           1.39x\n",
      "  B — with imputation               3.156     4.118           1.30x\n",
      "\n",
      "  Note: IV > OLS indicates measurement error dominates simultaneity bias,\n",
      "  consistent with Hall & Jones (1999) finding (OLS=3.29, 2SLS=5.14).\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 5 — TABLE II: IV REGRESSIONS\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\"  TABLE II — OLS and 2SLS Regressions\")\n",
    "print(\"  Dependent variable: log(Y/L)\")\n",
    "print(\"  Key regressor: Social infrastructure S ∈ [0,1]\")\n",
    "print(\"  Robust (HC1) standard errors in parentheses\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "def run_regressions(sample, label):\n",
    "    s = sample.copy().dropna(\n",
    "        subset=['log_yl','social_infra'] + INSTRUMENTS\n",
    "    )\n",
    "    n = len(s)\n",
    "\n",
    "    y  = s['log_yl'].values\n",
    "    S  = s['social_infra'].values.reshape(-1, 1)\n",
    "    X1 = np.ones((n, 1))                           # just constant\n",
    "    Z  = s[INSTRUMENTS].values\n",
    "\n",
    "    # ── OLS ──\n",
    "    X_ols  = np.column_stack([X1, S])\n",
    "    res_ols = ols(y, X_ols)\n",
    "\n",
    "    # ── 2SLS (all four instruments) ──\n",
    "    res_iv = tsls(y, S, X1, Z)\n",
    "\n",
    "    print(f\"\\n  [{label}]  N = {n}\")\n",
    "    print(f\"\\n  OLS:\")\n",
    "    print(fmt_result(res_ols, ['Constant', 'Social infra (S)']))\n",
    "    print(f\"\\n  2SLS (instruments: dist_eq, FR trade, English frac, WE lang frac):\")\n",
    "    print(fmt_result(res_iv, ['Constant', 'Social infra (S)']))\n",
    "\n",
    "    # ── 2SLS with subsets of instruments (sensitivity) ──\n",
    "    print(f\"\\n  2SLS sensitivity — instrument subsets:\")\n",
    "    subsets = [\n",
    "        (['distancefromeq'],                               \"dist_eq only\"),\n",
    "        (['distancefromeq','fr_trade'],                    \"dist + FR trade\"),\n",
    "        (['distancefromeq','english_frac','we_lang_frac'], \"dist + language\"),\n",
    "        (INSTRUMENTS,                                      \"all 4 instruments\"),\n",
    "    ]\n",
    "    print(f\"  {'Instruments':<35} {'β̂ (S)':>8}  {'SE':>8}  {'F-stat':>8}  {'Overid p':>9}\")\n",
    "    print(f\"  {'-'*75}\")\n",
    "    for inst_list, desc in subsets:\n",
    "        Z_sub = s[inst_list].values\n",
    "        r = tsls(y, S, X1, Z_sub)\n",
    "        se_s = np.sqrt(r['V'][1, 1])\n",
    "        oid  = f\"{r['overid_p']:.3f}\" if not np.isnan(r['overid_p']) else \"  n/a\"\n",
    "        print(f\"  {desc:<35} {r['beta'][1]:>8.3f}  {se_s:>8.4f}  {r['F_first']:>8.2f}  {oid:>9}\")\n",
    "\n",
    "    return res_ols, res_iv, s\n",
    "\n",
    "print()\n",
    "ols_A, iv_A, sA_reg = run_regressions(sA, \"Sample A — complete cases\")\n",
    "ols_B, iv_B, sB_reg = run_regressions(sB, \"Sample B — with imputation\")\n",
    "\n",
    "# ── Compare OLS vs IV coefficients explicitly ──\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\"  OLS vs 2SLS comparison (Social infrastructure coefficient β)\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"\\n  {'Sample':<30} {'OLS β':>8}  {'2SLS β':>8}  {'Ratio IV/OLS':>13}\")\n",
    "print(f\"  {'-'*65}\")\n",
    "for label, r_ols, r_iv in [(\"A — complete cases\", ols_A, iv_A),\n",
    "                             (\"B — with imputation\", ols_B, iv_B)]:\n",
    "    b_ols = r_ols['beta'][1]\n",
    "    b_iv  = r_iv['beta'][1]\n",
    "    print(f\"  {label:<30} {b_ols:>8.3f}  {b_iv:>8.3f}  {b_iv/b_ols:>13.2f}x\")\n",
    "print()\n",
    "print(\"  Note: IV > OLS indicates measurement error dominates simultaneity bias,\")\n",
    "print(\"  consistent with Hall & Jones (1999) finding (OLS=3.29, 2SLS=5.14).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90eddde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating Figure I ---\n",
      "  Saved to C:\\Users\\Adams\\OneDrive\\DE & E Research\\outputs\\figure_I_yl_vs_si.png\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 6 — FIGURE I: Y/L vs Social Infrastructure\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "print(\"\\n--- Generating Figure I ---\")\n",
    "\n",
    "# Highlight a selection of countries with labels\n",
    "LABEL_ISOS = {\n",
    "    'USA', 'CHE', 'NOR', 'JPN', 'GBR', 'FRA', 'DEU', 'CAN', 'AUS',\n",
    "    'BRA', 'MEX', 'ARG', 'COL', 'THA', 'MYS', 'KOR',\n",
    "    'CHN', 'IND', 'PAK', 'BGD', 'NGA', 'KEN', 'ETH', 'NER', 'ZAF',\n",
    "    'EGY', 'GHA', 'TZA', 'UGA', 'SEN',\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle(\"Figure I — Output per Worker vs. Social Infrastructure\",\n",
    "             fontsize=13, fontweight='bold', y=1.01)\n",
    "\n",
    "for ax, sample, title in [\n",
    "    (axes[0], sA_reg, \"Sample A: Complete cases (N={})\".format(len(sA_reg))),\n",
    "    (axes[1], sB_reg, \"Sample B: With imputation (N={})\".format(len(sB_reg))),\n",
    "]:\n",
    "    s = sample.dropna(subset=['log_yl', 'social_infra'])\n",
    "\n",
    "    ax.scatter(s['social_infra'], s['log_yl'],\n",
    "               color='steelblue', alpha=0.55, s=30, zorder=2, linewidths=0)\n",
    "\n",
    "    # Fitted OLS line\n",
    "    xi = np.linspace(s['social_infra'].min(), s['social_infra'].max(), 200)\n",
    "    X_fit = np.column_stack([np.ones(200), xi])\n",
    "    y_fit = X_fit @ ols(s['log_yl'].values,\n",
    "                        np.column_stack([np.ones(len(s)), s['social_infra'].values]))['beta']\n",
    "    ax.plot(xi, y_fit, color='firebrick', linewidth=1.8, zorder=3, label='OLS fit')\n",
    "\n",
    "    # Country labels\n",
    "    for _, row in s.iterrows():\n",
    "        if row['iso3'] in LABEL_ISOS:\n",
    "            ax.annotate(row['iso3'],\n",
    "                        xy=(row['social_infra'], row['log_yl']),\n",
    "                        xytext=(3, 1), textcoords='offset points',\n",
    "                        fontsize=6.5, color='#333333', zorder=4)\n",
    "\n",
    "    ax.set_xlabel(\"Social Infrastructure (S)\", fontsize=11)\n",
    "    ax.set_ylabel(\"log(Output per Worker)\", fontsize=11)\n",
    "    ax.set_title(title, fontsize=10, pad=6)\n",
    "    ax.set_xlim(-0.02, 1.05)\n",
    "    ax.grid(True, alpha=0.25, linewidth=0.5)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    # Annotate with IV coefficient\n",
    "    iv_res = iv_A if sample is sA_reg else iv_B\n",
    "    b  = iv_res['beta'][1]\n",
    "    se = np.sqrt(iv_res['V'][1, 1])\n",
    "    ax.text(0.03, 0.96, f\"2SLS β = {b:.2f} ({se:.3f})\",\n",
    "            transform=ax.transAxes, fontsize=9,\n",
    "            verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white',\n",
    "                      edgecolor='#cccccc', alpha=0.9))\n",
    "\n",
    "plt.tight_layout()\n",
    "fig1_path = OUT_DIR + 'figure_I_yl_vs_si.png'\n",
    "plt.savefig(fig1_path, dpi=160, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"  Saved to {fig1_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9e1d555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Figure II ---\n",
      "  Saved to C:\\Users\\Adams\\OneDrive\\DE & E Research\\outputs\\figure_II_tfp_vs_si.png\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 7 — FIGURE II: TFP vs Social Infrastructure\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "print(\"--- Generating Figure II ---\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle(\"Figure II — TFP (Productivity) vs. Social Infrastructure\",\n",
    "             fontsize=13, fontweight='bold', y=1.01)\n",
    "\n",
    "for ax, sample, title in [\n",
    "    (axes[0], sA_enriched, \"Sample A: Complete cases (N={})\".format(len(sA_enriched.dropna(subset=['log_tfp','social_infra'])))),\n",
    "    (axes[1], sB_enriched, \"Sample B: With imputation (N={})\".format(len(sB_enriched.dropna(subset=['log_tfp','social_infra'])))),\n",
    "]:\n",
    "    s = sample.dropna(subset=['log_tfp', 'social_infra']).copy()\n",
    "    s = s[np.isfinite(s['log_tfp']) & np.isfinite(s['social_infra'])]\n",
    "\n",
    "    ax.scatter(s['social_infra'], s['log_tfp'],\n",
    "               color='darkorange', alpha=0.55, s=30, zorder=2, linewidths=0)\n",
    "\n",
    "    # Fitted OLS line\n",
    "    xi = np.linspace(s['social_infra'].min(), s['social_infra'].max(), 200)\n",
    "    X_fit = np.column_stack([np.ones(200), xi])\n",
    "    y_fit = X_fit @ ols(s['log_tfp'].values,\n",
    "                        np.column_stack([np.ones(len(s)), s['social_infra'].values]))['beta']\n",
    "    ax.plot(xi, y_fit, color='firebrick', linewidth=1.8, zorder=3)\n",
    "\n",
    "    # Country labels\n",
    "    for _, row in s.iterrows():\n",
    "        if row['iso3'] in LABEL_ISOS:\n",
    "            ax.annotate(row['iso3'],\n",
    "                        xy=(row['social_infra'], row['log_tfp']),\n",
    "                        xytext=(3, 1), textcoords='offset points',\n",
    "                        fontsize=6.5, color='#333333', zorder=4)\n",
    "\n",
    "    # Correlation\n",
    "    r, p_val = stats.pearsonr(s['social_infra'], s['log_tfp'])\n",
    "    ax.text(0.03, 0.96, f\"r = {r:.3f}\",\n",
    "            transform=ax.transAxes, fontsize=9,\n",
    "            verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white',\n",
    "                      edgecolor='#cccccc', alpha=0.9))\n",
    "\n",
    "    ax.set_xlabel(\"Social Infrastructure (S)\", fontsize=11)\n",
    "    ax.set_ylabel(\"log(TFP)\", fontsize=11)\n",
    "    ax.set_title(title, fontsize=10, pad=6)\n",
    "    ax.set_xlim(-0.02, 1.05)\n",
    "    ax.grid(True, alpha=0.25, linewidth=0.5)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig2_path = OUT_DIR + 'figure_II_tfp_vs_si.png'\n",
    "plt.savefig(fig2_path, dpi=160, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"  Saved to {fig2_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55451ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================================================\n",
      "  DOCUMENTED DEVIATIONS FROM HALL & JONES (1999)\n",
      "=================================================================\n",
      "\n",
      "  [Price base year]\n",
      "    H&J use PWT 5.6 (1985 prices). We use PWT 10.01 (2017 prices).\n",
      "    Effect: Y/L levels differ in absolute terms; relative rankings robust.\n",
      "\n",
      "  [Governance data]\n",
      "    H&J use GADP (ICRG, 1986–1995 avg). We use WGI (rl+ge+cc avg, 1996).\n",
      "    Effect: Main risk is attenuation for countries with changing institutions;\n",
      "    institutional rankings highly correlated across time and source.\n",
      "\n",
      "  [Openness coverage]\n",
      "    H&J use Sachs-Warner 1950–1994. Our data ends 1992 (2-year gap).\n",
      "    Effect: Negligible — affects only 2 of 40+ years in the denominator.\n",
      "\n",
      "  [Mining value added]\n",
      "    H&J use 1988 values. We use 1990 (earliest available).\n",
      "    Effect: Negligible — mining shares are stable year to year.\n",
      "\n",
      "  [Sample size]\n",
      "    H&J N=127 (with imputation). Our N ≈ 113–120 depending on sample.\n",
      "    Effect: Some small/transition economies missing due to data gaps.\n",
      "\n",
      "  [Standard errors]\n",
      "    H&J use bootstrap (10,000 reps). We use HC1 robust standard errors.\n",
      "    Effect: SEs may differ slightly; coefficient estimates identical.\n",
      "\n",
      "=================================================================\n",
      "  Analysis complete. Output files:\n",
      "    C:\\Users\\Adams\\OneDrive\\DE & E Research\\outputs\\figure_I_yl_vs_si.png\n",
      "    C:\\Users\\Adams\\OneDrive\\DE & E Research\\outputs\\figure_II_tfp_vs_si.png\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# SECTION 8 — DEVIATIONS FROM ORIGINAL PAPER\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\"  DOCUMENTED DEVIATIONS FROM HALL & JONES (1999)\")\n",
    "print(\"=\" * 65)\n",
    "deviations = [\n",
    "    (\"Price base year\",\n",
    "     \"H&J use PWT 5.6 (1985 prices). We use PWT 10.01 (2017 prices).\\n\"\n",
    "     \"    Effect: Y/L levels differ in absolute terms; relative rankings robust.\"),\n",
    "    (\"Governance data\",\n",
    "     \"H&J use GADP (ICRG, 1986–1995 avg). We use WGI (rl+ge+cc avg, 1996).\\n\"\n",
    "     \"    Effect: Main risk is attenuation for countries with changing institutions;\\n\"\n",
    "     \"    institutional rankings highly correlated across time and source.\"),\n",
    "    (\"Openness coverage\",\n",
    "     \"H&J use Sachs-Warner 1950–1994. Our data ends 1992 (2-year gap).\\n\"\n",
    "     \"    Effect: Negligible — affects only 2 of 40+ years in the denominator.\"),\n",
    "    (\"Mining value added\",\n",
    "     \"H&J use 1988 values. We use 1990 (earliest available).\\n\"\n",
    "     \"    Effect: Negligible — mining shares are stable year to year.\"),\n",
    "    (\"Sample size\",\n",
    "     \"H&J N=127 (with imputation). Our N ≈ 113–120 depending on sample.\\n\"\n",
    "     \"    Effect: Some small/transition economies missing due to data gaps.\"),\n",
    "    (\"Standard errors\",\n",
    "     \"H&J use bootstrap (10,000 reps). We use HC1 robust standard errors.\\n\"\n",
    "     \"    Effect: SEs may differ slightly; coefficient estimates identical.\"),\n",
    "]\n",
    "for title, desc in deviations:\n",
    "    print(f\"\\n  [{title}]\")\n",
    "    print(f\"    {desc}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\"  Analysis complete. Output files:\")\n",
    "print(f\"    {fig1_path}\")\n",
    "print(f\"    {fig2_path}\")\n",
    "print(\"=\" * 65)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
